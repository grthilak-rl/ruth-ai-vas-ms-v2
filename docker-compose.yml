# Ruth AI Full System Docker Compose
# Phase 7 - I1: Full System Bring-Up (Baseline)
#
# This configuration starts all Ruth AI services together:
# - ruth-ai-backend: FastAPI backend API (port 8090 on host, 8080 internal)
# - ruth-ai-frontend: React UI served via nginx (port 3300)
# - ruth-ai-nlp-chat: NLP Chat microservice (port 8081)
# - ollama: LLM service for NLP (port 11434)
# - fall-detection-model: AI inference service (port 8010 on host, 8000 internal)
# - ppe-detection-model: PPE detection AI service (port 8011 on host, 8000 internal)
# - postgres: PostgreSQL database (port 5434 on host, 5432 internal)
# - redis: Redis cache (port 6382 on host, 6379 internal)
#
# External dependency: VAS-MS-V2 (configurable via VAS_BASE_URL env var)
#
# Usage:
#   docker compose up          # Start all services
#   docker compose up -d       # Start in background
#   docker compose logs -f     # Follow logs
#   docker compose down        # Stop all services
#
# Port Mapping (Ruth AI VAS-MS-V2 Integration Stack):
#   VAS Frontend:       3200 (external - already running)
#   Ruth AI Frontend:   3300 (this stack)
#   Ruth AI Backend:    8090 (this stack, host) -> 8080 (container)
#   Ruth AI NLP Chat:   8081 (this stack)
#   Ollama LLM:         11434 (this stack)
#   Fall Detection AI:  8010 (this stack, host) -> 8000 (container)
#   PPE Detection AI:   8011 (this stack, host) -> 8000 (container)
#   PostgreSQL:         5434 (this stack, host) -> 5432 (container)
#   Redis:              6382 (this stack, host) -> 6379 (container)
#   VAS Backend API:    8085 (external - already running)

services:
  # ============================================
  # PostgreSQL Database
  # ============================================
  postgres:
    image: postgres:15-alpine
    container_name: ruth-ai-vas-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-ruth}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-ruth_dev_password}
      POSTGRES_DB: ${POSTGRES_DB:-ruth_ai}
    ports:
      - "5434:5432"
    volumes:
      - ruth-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-ruth} -d ${POSTGRES_DB:-ruth_ai}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - ruth-ai-internal

  # ============================================
  # Redis Cache
  # ============================================
  redis:
    image: redis:7-alpine
    container_name: ruth-ai-vas-redis
    restart: unless-stopped
    ports:
      - "6382:6379"
    volumes:
      - ruth-redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      - ruth-ai-internal

  # ============================================
  # Fall Detection Model (AI Inference Service)
  # ============================================
  fall-detection-model:
    build:
      context: ./fall-detection-model
      dockerfile: Dockerfile
    container_name: ruth-ai-vas-fall-detection
    restart: unless-stopped
    ports:
      - "8010:8000"
    volumes:
      # Model weights are baked into the image from fall-detection-model/weights/
      - ./fall-detection-model/weights:/app/weights:ro
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=info
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Model loading takes time
    networks:
      - ruth-ai-internal
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # ============================================
  # PPE Detection Model (AI Inference Service)
  # Detects Personal Protective Equipment presence and violations
  # ============================================
  ppe-detection-model:
    build:
      context: ./ppe-detection-model
      dockerfile: Dockerfile
    container_name: ruth-ai-vas-ppe-detection
    restart: unless-stopped
    ports:
      - "8011:8000"
    volumes:
      # Model weights are baked into the image from ppe-detection-model/weights/
      - ./ppe-detection-model/weights:/app/weights:ro
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=info
      - AI_DEVICE=auto
      - PPE_CONFIDENCE_THRESHOLD=0.5
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # 12 models take longer to load
    networks:
      - ruth-ai-internal
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          memory: 4G

  # ============================================
  # Ruth AI Backend API
  # ============================================
  ruth-ai-backend:
    build:
      context: ./ruth-ai-backend
      dockerfile: Dockerfile
    container_name: ruth-ai-vas-backend
    restart: unless-stopped
    ports:
      - "8090:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]
    environment:
      # Application
      RUTH_AI_ENV: ${RUTH_AI_ENV:-development}
      RUTH_AI_LOG_LEVEL: ${RUTH_AI_LOG_LEVEL:-info}
      RUTH_AI_LOG_FORMAT: json
      # Server
      HOST: 0.0.0.0
      PORT: 8080
      # Database (using asyncpg driver)
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-ruth}:${POSTGRES_PASSWORD:-ruth_dev_password}@postgres:5432/${POSTGRES_DB:-ruth_ai}
      DATABASE_POOL_SIZE: 10
      DATABASE_POOL_OVERFLOW: 5
      # Redis
      REDIS_URL: redis://redis:6379/0
      REDIS_MAX_CONNECTIONS: 50
      # AI Runtime - pointing to AI model services
      AI_RUNTIME_URL: http://fall-detection-model:8000
      AI_RUNTIME_TIMEOUT_MS: 5000
      AI_RUNTIME_RETRY_COUNT: 3
      # PPE Detection Service
      PPE_DETECTION_URL: http://ppe-detection-model:8000
      PPE_DETECTION_TIMEOUT_MS: 10000
      PPE_DETECTION_RETRY_COUNT: 3
      # VAS Integration (external service)
      VAS_BASE_URL: ${VAS_BASE_URL:-http://10.30.250.245:8085}
      VAS_CLIENT_ID: ${VAS_CLIENT_ID:-ruth-ai-backend}
      VAS_CLIENT_SECRET: ${VAS_CLIENT_SECRET:-vas-portal-secret-2024}
      VAS_TOKEN_REFRESH_MARGIN_SEC: 300
      # JWT Authentication
      JWT_SECRET_KEY: ${JWT_SECRET_KEY:-ruth-ai-dev-jwt-secret-change-in-prod}
      JWT_ALGORITHM: HS256
      JWT_EXPIRY_MINUTES: 60
      # NLP Chat Service (optional - runs as separate microservice)
      NLP_CHAT_SERVICE_URL: http://ruth-ai-nlp-chat:8081
      NLP_CHAT_TIMEOUT_SECONDS: ${NLP_CHAT_TIMEOUT_SECONDS:-120}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      fall-detection-model:
        condition: service_healthy
      # NLP Chat is optional - backend starts regardless of NLP service status
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - ruth-ai-internal

  # ============================================
  # Ruth AI NLP Chat Service (Standalone Microservice)
  # Natural language queries using Ollama LLM
  # Can be stopped/started independently via /control/enable and /control/disable
  # ============================================
  ruth-ai-nlp-chat:
    build:
      context: ./ruth-ai-nlp-chat
      dockerfile: Dockerfile
      target: development
    container_name: ruth-ai-vas-nlp-chat
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      # Application
      NLP_ENV: ${RUTH_AI_ENV:-development}
      NLP_LOG_LEVEL: ${RUTH_AI_LOG_LEVEL:-info}
      HOST: 0.0.0.0
      PORT: 8081
      # Database (read-only queries to same DB as backend)
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-ruth}:${POSTGRES_PASSWORD:-ruth_dev_password}@postgres:5432/${POSTGRES_DB:-ruth_ai}
      # Ollama LLM Configuration
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_SQL_MODEL: ${OLLAMA_SQL_MODEL:-llama3.2:1b}
      OLLAMA_NLG_MODEL: ${OLLAMA_NLG_MODEL:-llama3.2:1b}
      OLLAMA_SQL_TEMPERATURE: ${OLLAMA_SQL_TEMPERATURE:-0.0}
      OLLAMA_NLG_TEMPERATURE: ${OLLAMA_NLG_TEMPERATURE:-0.3}
      OLLAMA_TIMEOUT_SECONDS: ${OLLAMA_TIMEOUT_SECONDS:-300}
      # Chat Configuration
      CHAT_MAX_RESULT_ROWS: ${CHAT_MAX_RESULT_ROWS:-100}
      CHAT_ALLOWED_TABLES: devices,stream_sessions,events,violations,evidence
      # Service Control (can be toggled via API)
      NLP_SERVICE_ENABLED: ${NLP_SERVICE_ENABLED:-true}
    depends_on:
      postgres:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ruth-ai-internal
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 256M

  # ============================================
  # Ollama LLM Service
  # Provides SQL generation and natural language response models
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: ruth-ai-vas-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ruth-ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - ruth-ai-internal
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # ============================================
  # Ollama Model Initialization
  # Automatically pulls required models on first startup
  # ============================================
  ollama-init:
    image: ollama/ollama:latest
    container_name: ruth-ai-vas-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling LLM models for NLP Chat..."
        ollama pull llama3.2:1b
        ollama pull anindya/prem1b-sql-ollama-fp116
        echo "Model pull complete!"
    environment:
      OLLAMA_HOST: ollama:11434
    networks:
      - ruth-ai-internal
    restart: "no"

  # ============================================
  # Ruth AI Frontend (React UI via nginx)
  # ============================================
  ruth-ai-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: ruth-ai-vas-frontend
    restart: unless-stopped
    ports:
      - "3300:80"
    environment:
      # VAS proxy URL for nginx (substituted at container startup)
      # Change VAS_BASE_URL in .env or via environment to point to your VAS server
      VAS_PROXY_URL: ${VAS_BASE_URL:-http://10.30.250.245:8085}
      # These are baked in at build time via Vite
      VITE_API_BASE_URL: http://localhost:8080
      VITE_VAS_WEBRTC_URL: ${VAS_WEBRTC_URL:-ws://10.30.250.245:3002}
    depends_on:
      ruth-ai-backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ruth-ai-internal
      - ruth-ai-frontend

# ============================================
# Networks
# ============================================
networks:
  ruth-ai-internal:
    name: ruth-ai-vas-internal
    driver: bridge
  ruth-ai-frontend:
    name: ruth-ai-vas-frontend-net
    driver: bridge

# ============================================
# Volumes
# ============================================
volumes:
  ruth-postgres-data:
    name: ruth-ai-vas-postgres-data
  ruth-redis-data:
    name: ruth-ai-vas-redis-data
  ruth-ollama-data:
    name: ruth-ai-vas-ollama-data